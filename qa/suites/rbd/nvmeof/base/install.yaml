use_shaman: True
tasks:
- cephadm:
- cephadm.shell:
    host.a:
    # get state before nvmeof deployment
    - ceph orch status
    - ceph orch ps
    - ceph orch ls
    - ceph orch host ls
    - ceph orch device ls
    - ceph osd lspools
    # create pool
    - ceph osd pool create mypool
    - ceph osd pool application enable mypool rbd
    - ceph osd lspools
    - ceph osd pool ls detail
    # deploy nvmeof
    - |
      set -ex
      #
      ceph config set mgr mgr/cephadm/container_image_nvmeof quay.io/ceph/nvmeof:0.0.3
      ceph config get mgr mgr/cephadm/container_image_nvmeof
      ceph orch apply nvmeof mypool --placement="1 $(hostname)"
    - ceph orch ps --refresh
    - ceph versions
    - ceph -s
    - ceph orch ls
    - rbd create mypool/myimage --size 8Gi
    - rbd ls mypool

- cephadm.wait_for_service:
    service: nvmeof.mypool

- exec:
    host.a:
    - |
      set -ex
      # print nvmeof start up logs
      s=$(systemctl list-units | grep nvmeof.mypool | awk '{print $1}')
      PAGER=cat journalctl -u $s

#- install:
#    extra_packages:
#    - nvme-cli

- exec:
    client.0:
    - "modprobe nvme-fabrics"
    - |
      set -ex
      #
      ip --brief address show
      IP=$(hostname -I)
      podman images
      podman ps
      echo "<---- $(hostname) $IP $(hostname -I) ---->"
      podman run -it quay.io/ceph/nvmeof-cli:0.0.3 --server-address $IP --server-port 5500 create_bdev --pool "mypool" --image "myimage" --bdev "mybdev"

