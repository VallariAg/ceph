roles:
- - host.a
  - mon.a
  - mgr.x
  - osd.0
  - osd.1
  - client.0
  - ceph.nvmeof.nvmeof.a
- - host.b
  - mon.b
  - osd.2
  - osd.3
  - osd.4
  - client.1
  - ceph.nvmeof.nvmeof.b
- - host.c
  - mon.c
  - osd.5
  - osd.6
  - osd.7
  - client.2
  - ceph.nvmeof.nvmeof.c
- - client.3 # initiator

overrides:
  ceph:
    conf:
      mon:
        mon op complaint time: 300
        # cephadm can take up to 5 minutes to bring up remaining mons
        mon down mkfs grace: 300
    log-ignorelist:
      - overall HEALTH_
      - \(MON_DOWN\)

tasks:
- nvmeof:
    client: client.0
    gw_image: quay.io/barakda1/nvmeof:1.2.3 # "default" is the image cephadm defaults to; change to test specific nvmeof images, example "latest"
    rbd:
      pool_name: mypool
      image_name_prefix: myimage
    gateway_config:
      subsystems_count: 3
      namespaces_count: 5
      cli_image: quay.io/barakda1/nvmeof-cli:1.2.3

- cephadm.wait_for_service:
    service: nvmeof.mypool

- workunit:
    no_coverage_and_limits: true
    clients:
      client.3:
        - rbd/nvmeof_setup_subsystem.sh
        - rbd/nvmeof_basic_tests.sh
    env:
      RBD_POOL: mypool
      RBD_IMAGE_PREFIX: myimage

- nvmeof.thrash:
    checker_host: 'client.3'
    switch_thrashers: True

- mon_thrash:
    revive_delay: 60
    thrash_delay: 60
    thrash_many: true
    switch_thrashers: True
    logger: '[nvmeof.thrasher.mon_thrasher]'

- workunit:
    no_coverage_and_limits: true
    timeout: 30m
    clients:
      client.3:
        - rbd/nvmeof_fio_test.sh --rbd_iostat
    env:
      RBD_POOL: mypool
      IOSTAT_INTERVAL: '10'
      RUNTIME: '600'
