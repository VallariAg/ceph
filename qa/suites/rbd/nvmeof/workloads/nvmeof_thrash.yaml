roles:
- - host.a
  - mon.a
  - mgr.x
  - osd.0
  - osd.1
  - client.0
  - ceph.nvmeof.nvmeof.a
- - host.b
  - mon.b
  - osd.2
  - osd.3
  - osd.4
  - client.1
  - ceph.nvmeof.nvmeof.b
- - host.c
  - mon.c
  - osd.5
  - osd.6
  - osd.7
  - client.2
  - ceph.nvmeof.nvmeof.c
- - client.3 # initiator

overrides:
  ceph:
    log-ignorelist:
      - overall HEALTH_
      - \(MON_DOWN\)

tasks:
- nvmeof:
    client: client.0
    gw_image: quay.io/barakda1/nvmeof:qe_ceph_devel_c1117be  # "default" is the image cephadm defaults to; change to test specific nvmeof images, example "latest"
    rbd:
      pool_name: mypool
      image_name_prefix: myimage
    gateway_config:
      subsystems_count: 3
      namespaces_count: 5
      cli_image: quay.io/barakda1/nvmeof-cli:qe_ceph_devel_c1117be

- cephadm.wait_for_service:
    service: nvmeof.mypool

- workunit:
    no_coverage_and_limits: true
    clients:
      client.3:
        - rbd/nvmeof_setup_subsystem.sh
        - rbd/nvmeof_basic_tests.sh
    env:
      RBD_POOL: mypool
      RBD_IMAGE_PREFIX: myimage

- nvmeof.thrash:
    checker_host: 'client.3'

# - mon_thrash:
#     revive_delay: 60
#     thrash_delay: 60
#     thrash_many: true
#     give_up_control: True

- workunit:
    no_coverage_and_limits: true
    timeout: 30m
    clients:
      client.3:
        - rbd/nvmeof_fio_test.sh --rbd_iostat
    env:
      RBD_POOL: mypool
      IOSTAT_INTERVAL: '10'
      RUNTIME: '600'
